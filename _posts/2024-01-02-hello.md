---
title: Reinforcement Learning
layout: post
math: true
---

## Reinforcement Learning
Reinforcement Learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal. The learner isnt told what to do, rather *explores* the environment by itself. One action doesnt only affect current reward but also affects whats the next situation and hence all *future rewards*. These are two characteristics features of RL - delayed reward and exploration.

The underlying mathematical concept of RL is Markov Decision Chains.

## Markov Decision Process

The MDP for RL is denoted by the tuple $(S,A,R)$ (States,Actions,Rewards).

![MDP](/assets/images/MDP.png)

The loop of RL is as follows - 
- The environment gives the agent a State $S_t$ .
- The Agent performs an action $A_t$ according to the state .
- The environment gives the agent the next state $S_t$ and the reward for the action taken $R_t$ .

All this is stored in agents action buffer, and will be used later to learn.

In a finite MDP, size of states,actions and rewards is finite. Also the random variable $R_t$ nad $S_t$ have well defined disceret probability distributions dependent only on preceding state and actions. This the one of the defining property of [Markov Chains](https://en.wikipedia.org/wiki/Markov_chain#Principles).



$$p(s',r|s,a) = Pr\{S_t = s', R_t = r,|S_{t-1} = s,A_{t-1} = a\}$$


The probability of getting state **s'** and reward **r**, only depends on previous state being **s** and the action taken being **a** and all the information needed to take the action **a** is given in the state **s**.

This p function defines the dynamics of the MDP.
The function can be used to calculate all other functions of the MDP.

- State-transition probabilities - probability of going from one state to another

$$p(s'|s,a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum\limits_{r \in R} {p(s',r|s,a)}$$

- Expected rewards for state action pair - Expected reward from taking action a at state s.

$$r(s,a) = \mathbb{E}[R_t|S_{t-1} = s,A_{t-1} = a] = \sum\limits_{r \in R}r{\sum\limits_{s' \in S}p(s',r|s,a)}$$

- Expected rewards for stae-action-next state triples. - Reward from taking action from state and going to state s' . 

$$r(s,a,s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum\limits_{r \in R}r\frac{p(s',r|s,a)}{p(s'|s,a)}$$

*If the state s' is fixed for each s,a pair, p(s'\|s,a) = 1, hence the formula changes to same as expected rewards for state action pair*

## Goals
Informally speaking the agent's goal is to maximize the total amount of rewards it receives. This means maximizing not immediate reward,but cummulative reward in the long run. We discussed this previously as delayed return.


**Reward hypothesis** - That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).

Rewards play an very important in RL as you can see from above defination.A lot of time goes into designing rewards functions which accurately represent the problem we are trying to solve.
If we want it to do somethingfor us, we must provide positive rewards to it in such a way that in maximizing them the agent will also acheve our goals.

One thing to remember while making the rewards is _rewards are a way of communicating to the agent **what** you want to achiece now **how** you want it achieved._

We denote return by $G_t$.

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + .... + R_T$$
<center>T denotes the final time step.</center>

> $$G_t = R_{t+1} + \gamma G_{t+1}$$
{: .prompt-info}


Let's suppouse we dealing with an infinite task, how would we model it using this. As we can see the term $G_t$ is not a converging sequence. To prevent that we introduce a weight term.

$$G_t = R_{t+1} + \gamma R_{t+2} +\gamma^2R_{t+3} + ... = \sum\limits_{k=0}^{\infty}\gamma^kR_{t+k+1}$$

## Example
Lets take up an example to see MDP in action.

 > A mobile robot has the job of collecting empty soda cans in an office environment. It has sensors for detecting cans, and an arm and gripper that can pick them up and place them in an onboard bin; it runs on a rechargeable battery. The robot's control system has components for interpreting sensory information, for navigating, and for controlling the arm and gripper. High-level decisions about how to search for cans are made by a reinforcement learning agent based on the current charge level of the battery. This agent has to decide whether the robot should (1) actively search for a can for a certain period of time, (2) remain stationary and wait for someone to bring it a can, or (3) head back to its home base to recharge its battery. This decision has to be made either periodically or whenever certain events occur, such as finding an empty can. The agent therefore has three actions, and its state is determined by the state of the battery. The rewards might be zero most of the time, but then become positive when the robot secures an empty can, or large and negative if the battery runs all the way down. In this example, the reinforcement learning agent is not the entire robot. The states it monitors describe conditions within the robot itself, not conditions of the robot's external environment. The agent's environment therefore includes the rest of the robot, which might contain other complex decision-making systems, as well as the robot's external environment.

 We will now try to convert this abstract problem to a MDP.

 As discussed before the MDP is defined by throuple (S,A,R). We will define each of them now.

 $$S = \{ low,high \} $$

 $$A(high) = \{search,wait\}$$

 $$A(low) = \{search,wait,recharge\}$$

Now we will design the dynamics of the MDP i.e the function p.

Lets define (s,a,s') throuples as doing action a at state s and going to state s'.
- (high-search-high) happens with probability $\alpha$ and reward $R^{search}$
- (high-search-low) happens with probability $1-\alpha$ and reward $R^{search}$
- (low-search-high) happens with probability $1-\beta$ and reward $-3$
- (low-search-low) happens with probability $\beta$ and reward $R^{search}$
- (high-wait-high) happens with probability $1$ and reward $R^{wait}$
- (high-wait-low) happens with probability $0$ and reward $R^{wait}$
- (low-wait-high) happens with probability $0$ and reward $R^{wait}$
- (low-wait-low) happens with probability $1$ and reward $R^{wait}$
- (low-recharge-high) happens with probability $1$ and reward $0$
- (low-recharge-low) happens with probability $0$ and reward $0$

The transition graph for the same would be - 
![graph](/assets/images/graph.png)


Now we have setup the basics of RL, we now know how to take an abstract problem and convert it into a MDP. In the next post we will look into how to go about solving the problem, i.e finding the optimal actions to solve the problem.

