---
title: Reinforcement Learning
layout: post
math: true
---

# Reinforcement Learning
Reinforcement Learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal. The learner isnt told what to do, rather *explores* the environment by itself. One action doesnt only affect current reward but also affects whats the next situation and hence all *future rewards*. These are two characteristics features of RL - delayed reward and exploration.

The underlying mathematical concept of RL is Markov Decision Chains.

## Markov Decision Process

The MDP for RL is denoted by the tuple $(S,A,R)$ (States,Actions,Rewards).

![MDP](/assets/images/MDP.png)

The loop of RL is as follows - 
- The environment gives the agent a State $S_t$ .
- The Agent performs an action $A_t$ according to the state .
- The environment gives the agent the next state $S_t$ and the reward for the action taken $R_t$ .

All this is stored in agents action buffer, and will be used later to learn.

In a finite MDP, size of states,actions and rewards is finite. Also the random variable $R_t$ nad $S_t$ have well defined disceret probability distributions dependent only on preceding state and actions. This the one of the defining property of [Markov Chains](https://en.wikipedia.org/wiki/Markov_chain#Principles).



$$p(s',r|s,a) = Pr\{S_t = s', R_t = r,|S_{t-1} = s,A_{t-1} = a\}$$


The probability of getting state **s'** and reward **r**, only depends on previous state being **s** and the action taken being **a** .

This p function defines the dynamics of the MDP.
The function can be used to calculate all other functions of the MDP.

- State-transition probabilities - probability of going from one state to another

$$p(s'|s,a) = Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum\limits_{r \in R} {p(s',r|s,a)}$$

- Expected rewards for state action pair - Expected reward from taking action a at state s.

$$r(s,a) = \mathbb{E}[R_t|S_{t-1} = s,A_{t-1} = a] = \sum\limits_{r \in R}r{\sum\limits_{s' \in S}p(s',r|s,a)}$$

- Expected rewards for stae-action-next state triples. - Reward from taking action from state and going to state s' . 

$$r(s,a,s') = \mathbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum\limits_{r \in R}r\frac{p(s',r|s,a)}{p(s'|s,a)}$$

*If the state s' is fixed for each s,a pair, p(s'\|s,a) = 1, hence the formula changes to same as expected rewards for state action pair*

## Goals
Informally speaking the agent's goal is to maximize the total amount of rewards it receives. This means maximizing not immediate reward,but cummulative reward in the long run. We discussed this previously as delayed return.


**Reward hypothesis** - That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).

Rewards play an very important in RL as you can see from above defination.A lot of time goes into designing rewards functions which accurately represent the problem we are trying to solve.
If we want it to do somethingfor us, we must provide positive rewards to it in such a way that in maximizing them the agent will also acheve our goals.

One thing to remember while making the rewards is _rewards are a way of communicating to the agent **what** you want to achiece now **how** you want it achieved._

We denote return by $G_t$.

$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + .... + R_T$$

T denotes the final time step.
Let's suppouse we dealing with an infinite task, how would we model it using this. As we can see the term $G_t$ is not a converging sequence. To prevent that we introduce a weight term.

$$G_t = R_{t+1} + \gamma R_{t+2} +\gamma^2R_{t+3} + ... = \sum\limits_{k=0}^{\infty}\gamma^kR_{t+k+1}$$